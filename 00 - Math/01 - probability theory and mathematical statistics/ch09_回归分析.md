# 1. 回归分析的概念
设随机变量 $X$ 与 $Y$ 之间存在某种相关关系, 其中 $X$ 是可控制的变量, $Y$ 是可观测的随机变量。变量 $X$ 影响变量 $Y$ , 但不能完全决定变量 $Y$,因为还存在其他随机因素(定义为随机误差 $\varepsilon$ )。
当 $X$ 取定一个数值 $x$ 时,可能有多个 $Y$ 的值与之对应,对 $Y$ 取平均,即令
$$
E(Y|X=x) =f(x)
$$
从而其他随机因素引起的偏差是
$$
\varepsilon = Y-f(x)
$$
此时 $X$ 与 $Y$ 的不确定性关系表示为
$$
Y=E(Y|X=x) +\varepsilon = f(x) + \varepsilon
$$
通常假定
$$
\varepsilon\sim N(0,\sigma^2)
$$

称 $f(x)$ 为回归函数。
回归分析的任务就是根据 $X$ 的取定的一组值 $x_1,x_2,\cdots,x_n$ 和 $Y$ 的观测值 $y_1,y_2,\cdots,y_n$ 去估计回归函数 $f(x)$, 以及讨论与此有关的种种统计推断问题。


# 2. 一元线性回归
如果样本数据 $(x_i, y_i), i=1,2\cdots,n$ 呈现在一条直线附近, 说明变量 $X$ 不能完全确定变量 $Y$ , 当 $X$ 与 $Y$ 之间具有明显的线性相关关系。其他因素对变量 $Y$ 的影响很小, 则将它归并在 $\varepsilon$ 中, 于是抽象为如下的一元线性模型
$$
\begin{cases}
y_i=\beta_0 + \beta_1x_i +e_i, & i=1,2,\cdots, n\\
\varepsilon_i\sim N(0, \sigma^2), & i=1,2,\cdots,n \\
\varepsilon_1, \varepsilon_2,\cdots, \varepsilon_n 相互独立
\end{cases}
$$
## 2.1 回归系数的最小二乘估计
回归分析的主要任务是通过样本观测值 $(x_i, y_i), i=1,2,\cdots,n$ 对参数 $\beta_0, \beta_1$ 进行估计。参数的估计表示为 $\hat{\beta_0},\hat{\beta_1}$, 称
$$
\hat{y} = \hat{\beta_0} + \hat{\beta_1} x 
$$
为 $y$ 关于 $x$ 的一元线性经验回归方程,在直角坐标西下也称为经验回归直线。

通常采用最小二乘法估计参数 $\beta_0, \beta_1$, 其思想是误差 $\varepsilon_i=y_i-(\beta_0+\beta_1x_i), i=1,2,\cdots, n$ 的平法和越小越好,记
$$
Q(\beta_0, \beta_1) = \sum_{i=1}^n\varepsilon_i^2=\sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2
$$
则参数 $\beta_0, \beta_1$ 的估计 $\hat{\beta_0}, \hat{\beta_1}$ 满足 $Q(\hat{\beta_0}, \hat{\beta_1}) = \min_{\beta_0, \beta_1}Q(\beta_0, \beta_1)$, 且称 $\hat{\beta_0}, \hat{\beta_1}$ 为回归系数 $\beta_0, \beta_1$ 的最小二乘估计。

对 $Q$ 求偏导整理后,有
$$
\begin{cases} 
n\beta_0 + \beta_1\sum_{i=1}^n x_i = \sum_{i=1}^ny_i \\
\beta_0 \sum_{i=1}^nx_i+\beta_1\sum_{i=1}^nx_i^2=\sum_{i=1}^nx_iy_i
\end{cases}
$$
或
$$
\begin{cases}
n\beta_0+n\overline{x} \beta_1 = n\overline{y} \\
n\overline{x} \beta_0+\beta_1\sum_{i=1}^nx_i^2 = \sum_{i=1}^nx_iy_i
\end{cases}
$$
上述方程称为正则方程组,方程组的解记为:
$$
\begin{cases}
\hat{\beta_0} = \overline{y} - \hat{\beta_1} \overline{x} \\
\hat{\beta_1} = \frac{\sum_{i=1}i^nx_iy_i-n\overline{x}\overline{y}}{\sum_{i=1}^nx_i^2-n\overline{x}^2}
\end{cases}
$$
通常有如下约定
$$
l_{xy} = \sum_{i=1}^n(x_i-\overline{x})(y_i-\overline{y}) \\
l_{xx} = \sum{i=1}^n(x_i-\overline{x})^2    \\
l_{yy} = \sum{i=1}^n(y_i-\overline{y})^2    \\
$$

则正则方程组的解可简记为
$$
\begin{cases}
\hat{\beta_1} = \frac{l_{xy}}{l_{xx}} \\
\hat{\beta_0} = \overline{y} - \hat{\beta_1} \overline{x}
\end{cases}
$$

为了便于计算机计算, 将一元线性回归模型写成:
$$
\begin{cases}
\vec{y} = \vec{X} \vec{\beta} +\vec{\varepsilon} \\
\vec{\varepsilon} \sim N_n(\vec{0}, \sigma^2\vec{I_n})
\end{cases}
$$
其中, $\vec{y}= (y_1,y_2,\cdots, y_n)^T$, $\vec{X}=\begin{bmatrix} 1&1&\cdots 1 \\ x_1& x_2&\cdots & x_n\end{bmatrix}$, $\vec{\beta} = (\beta_0, \beta_1)^T$, $\vec{\varepsilon} = (\varepsilon_, \varepsilon_2,\cdots,\varepsilon_n)^T$, $I_n$ 为 $n$ 阶单位矩阵。

则正则方程可表示为
$$
(\vec{X}^T\vec{X})\vec{\beta} = \vec{X}^T\vec{y}
$$
如果矩阵 $\vec{X}^T\vec{X}$ 的逆矩阵存在,则向量 $\vec{\beta}$ 的最小二乘的解可表示为
$$
\hat{\vec{\beta}} = (\vec{X}^T\vec{X})^{-1}\vec{X}^T\vec{y}
$$

## 2.2 最小二乘的性质
> @Property
> **性质1**
> 1. $\hat{\beta_0}\sim N(\beta_0, (\frac{1}{n}+\frac{\overline{x}^2}{l_{xx}})\sigma^2)$
> 2. $\hat{\beta_1} \sim N(\beta_1, \frac{\sigma^2}{l_{xx}})$
> 3. $\text{cov}(\hat{\beta_0}, \hat{\beta_1}) = -\frac{\overline{x}}{l_{xx}}\sigma^2$


> @Property
> **性质2**
> $\hat{y}= \hat{\beta_0}+\hat{\beta_1}x\sim N(\beta_0+ \beta_1x, (\frac{1}{n}+\frac{(x-\overline{x})^2}{l_{xx}})\sigma^2)$



# 2.3 显著性检验
使用经验回归直线前需要对 $Y$ 与 $X$ 之间的线性相关关系、经验回归直线拟合效果进行检验。
当 $|\beta_1|\neq 0$ 时, 认为 $Y$ 与 $X$ 之间有线性相关,则提出如下统计假设
$$
H_0:\beta_1=0, H_1:\beta_1\neq 0
$$
有如下检验方法:

### 2.3.1 $t$ 检验法

因为 $\hat{\beta_1} \sim N(\beta_1, \frac{\sigma^2}{l_{xx}})$ , 即 $\frac{(\hat{\beta_1}-\beta)\sqrt{l_{xx}}}{\sigma}\sim N(0,1)$。而 $\sigma^2$ 未知, 用 $\hat{\sigma}=\sqrt{\frac{S_E^2}{n-2}}$ 估计参数 $\sigma$, 得到统计量
$$
T=\frac{\hat{\beta_1}\sqrt{l_{xx}}}{\hat{\sigma}}
$$
且在 $H_0$ 成立的条件下, $T\sim t(n-2)$, 因此拒绝域为
$$
\mathscr{X}_0 =\{|t|> t_{1-\frac{\alpha}{2}}(n-2)\}
$$

### 2.3.2 $r$ 检验法
样本相关系数 $r=\frac{l_{xy}}{\sqrt{l_{xx}l_{yy}}}$ 是参数 $\rho(X,Y)$ 的估计,因此选择统计量为 $r$ , 当 $|r|\to 1$ 时, $X$ 与 $Y$ 具有较强的线性相关关系,则 $H_0$ 不成立, 因此 $H_0$ 的拒绝域为
$$
R=\{|r|>r_\alpha(n-2)\}
$$

### 2.3.3 $F$ 检验法
由平方和公式
$$
\sum_{i=1}^n(y_i-\overline{x})^2=\sum_{i=1}^n(y_1-\hat{y_i})^2 + \sum_{i=1}^n(\hat{y_i}-\overline{y})^2
$$
其中记 $S_T^2= \sum_{i=1}^n(y_i-\overline{x})^2$, $S_E^2=\sum_{i=1}^n(y_1-\hat{y_i})^2 $ , $S_R^2=\sum_{i=1}^n(\hat{y_i}-\overline{y})^2$ ,它们分别表示总离差平方和、残差平方和、回归平方和。

回归平方和 $S_R^2$ 的值相对于 $S_E^2$ 越大,反映 $Y$ 对 $X$ 的回归效果越好,于是选择统计量 $\frac{S_R^2}{S_E^2}$ 。

先探讨 $S_E^2$ 与 $S_R^2$ 的性质
> @Definition
> **性质**
> 1. $S_E^2$ 与 $S_R^2$ 独立
> 2. $\frac{S_E^2}{\sigma^2}\sim \chi^2(n-2)$
> 3. 当 $\beta_1=0$ 时, $\frac{S_R^2}{\sigma^2}\sim \chi^2(1)$


于是在 $H_0$ 成立的条件下:
$$
F=\frac{S_R^2}{S_E^2/(n-2)}\sim F(1, n-2)
$$

$H_0$ 的拒绝域为
$$
\mathscr{X}_0 =\{f>F_{1-\alpha}(1,n-2)\}
$$

## 2.4 预测与控制
### 2.4.1 预测

预测可分为
1. 点预测
    对给定的 $X=x_0$ , 预测对应 $Y_0$ 的值, 即用 $\hat{y_0}=\hat{\beta_0}+\hat{\beta_1}x_0$ 作为 $Y_0$ 的预测值
2. 区间预测
   在一定的置信度下预测 $Y_0$ 的取值范围, 可得 $Y_0$ 在置信度为 $1-\alpha$ 下的置信(预测) 区间为
   $$
   (\hat{y_0}-\hat{\sigma}S(x_0)t_{1-\frac{\alpha}{2}}(n-2), \hat{y_0}+\hat{\sigma}S(x_0)t_{1-\frac{\alpha}{2}}(n-2))
   $$
   其中, $\hat{\sigma}=\sqrt{\frac{S_E^2}{n-2}}$, $S(x_0)=\sqrt{1+\frac{1}{n}+\frac{(x_0-\overline{x})^2}{l_{xx}}}$


### 2.4.2 控制
控制问题:求出自变量 $x$ 的取值区间 $(x_1,x_2)$ 使得对应的应变量 $Y$ 以 $1-\alpha$ 的概率落在 $(y_1,y_2)$ 内。
当样本容量很大, 且 $x$ 在 $\overline{x}$ 附近时, 指定的区间 $(y_1,y_2)$ 满足 $|y_2-y_1|>2\hat{\sigma}u_{1-\frac{\alpha}{2}}$,令
$$
y_1 = \hat{y_1}-\hat{\sigma} u_{1-\frac{\alpha}{2}} +\hat{\beta_1}x_1 \\
y_2 = \hat{y_2}+\hat{\sigma} u_{1-\frac{\alpha}{2}} +\hat{\beta_1}x_2 \\
$$
从中解出
$$
x_1 = \frac{1}{\hat{\beta_1}}(y_1-\hat{\beta_0}+\hat{\sigma}u_{1-\frac{\alpha}{2}}) \\ 
x_2 = \frac{1}{\hat{\beta_1}}(2_1-\hat{\beta_0}+\hat{\sigma}u_{1-\frac{\alpha}{2}}) \\ 
$$

# 3. 一元非线性回归
一些非线性函数可以通过适当的变量变换将它们转为线性函数形式:

|  函数名称  |             函数表达式              |               线性化方法及模型                |
| :--------: | :---------------------------------: | :-------------------------------------------: |
|  双曲函数  |  $\frac{1}{y}=a+\frac{b}{x}, a>0$   | $u=\frac{1}{y}, v=\frac{1}{x}$ <br />$u=a+bv$ |
|   幂函数   |           $y=ax^b (a>0)$            |     $u=\ln y, u=\ln x$ <br />$u=\ln a+bv$     |
|  指数函数  |          $y=ae^{bx} (a>0)$          |         $u=\ln y$<br />$u=\ln a + bx$         |
|  指数函数  |      $y=ae^{\frac{b}{x}}(a>0)$      |  $u=\ln y , v=\frac{1}{x}$<br />$u=\ln a+bv$  |
|  对数函数  |            $y=a+b\ln x$             |           $v =\ln x$<br />$y=a+bv$            |
| $S$ 型曲线 | $y=\frac{1}{a+be^{-x}}, (a>0, b>0)$ |    $u=\frac{1}{y}, v=e^{-x}$<br />$u=a+bv$    |

# 4. 多元线性回归模型简介

1. 一般的,多元回归函数 $f(x_1,x_2,\cdots,x_n)$ 表示为
   $$
   E(Y|X_1=x_2, X_2=x_2,\cdots, X_m=x_m) = f(x_1, x_2,\cdots, x_m)
   $$
   则回归模型为
   $$
   Y=f(x_1,x_2, \cdots, x_m)+\varepsilon
   $$
   多元线性回归模型如下
   $$
   Y=\beta_0+\sum_{i=1}^m\beta_ix_i+\varepsilon, \varepsilon\sim N(0,\sigma^2)
   $$

2. 如果获得 $n$ 组观测数据 $(x_{i1}, x_{i2},\cdots, x_{im}, y_i), i=1,2,\cdots, n$ ， 则线性回归模型可表示为
   $$
   \begin{cases}
   y_1 = \beta_0+\beta_1x_{11}+\beta_2x_{12}+\cdots+\beta_{m}x_{1m}+\varepsilon_1 \\
   y_2 = \beta_0+\beta_1x_{21}+\beta_2x_{22}+\cdots+\beta_{m}x_{2m}+\varepsilon_2 \\
   \qquad \cdots \cdots \\
   y_n = \beta_0+\beta_1x_{n1}+\beta_2x_{n2}+\cdots+\beta_{m}x_{nm}+\varepsilon_n \\
   \end{cases}
   $$
   ​	写成矩阵为
   $$
   \begin{cases}
   \vec{y} = \vec{X}\vec{\beta}+\vec{\varepsilon} \\
   \vec{\varepsilon} \sim N_n(\vec{0}, \sigma^2\vec{I}_n)
   \end{cases}
   $$
   ​	其中, $\vec{y} = (y_1,y_2,\cdots,y_n)^T$ , $\vec{\beta} = (\beta_0, \beta_1, \cdots,\beta_m)^T$, $\vec{\varepsilon} = (\varepsilon_1, \varepsilon_2, \cdots, \varepsilon_n)^T$ ,
   $$
   \vec{X}=
   \begin{bmatrix}
   1& x_{11}&\cdots& x_{1m} \\
   1& x_{21}&\cdots& x_{2m} \\
   \vdots& \vdots& \ddots & \vdots \\
   1& x_{n1}&\cdots& x_{nm} \\
   \end{bmatrix}
   $$
   ​	$\vec{I_n}$ 为 $n$ 阶单位矩阵。上述模型等价于 $\vec{y}\sim N_n(\vec{X}\vec{\beta}, \sigma^2\vec{I_n})$

3. 关于向量 $\vec{\beta}$ 的最小二乘估计可以表示为
   $$
   \hat{\vec{\beta}} = (\vec{X}^T\vec{X})^{-1}\vec{X}^T\vec{y}
   $$
   经验方程为
   $$
   y=\hat{\beta_0}+\hat{\beta_1}x_2+\cdots+\hat{\beta_m}x_m
   $$

4. 对于多元线性回归模型,通常采用 $F$ 检验法对回归效果进行检验

   检验问题:
   $$
   H_0:\beta_1=\beta_2=\cdots=\beta_m=0, H_1:\beta_i\neq 0 ,对某个 i
   $$
   分解公式:
   $$
   \sum_{i=1}^n(y_i-\overline{y})^2=\sum_{i=1}^n(y_i-\hat{y})^2+\sum_{i=1}^n(\hat{y_i}-\overline{y})^2
   $$
   即 $S_T^2=S_E^2+S_R^2$ 

5. 可以证明:

   - $S_E^2, S_R^2$ 独立
   - $S_E^2\sim \chi^2(n-m-1)$
   - 在 $H_0$ 下, $S_R^2\sim \chi^2(m)$

   构造统计量
   $$
   F=\frac{S_R^2/m}{S_E^2/(n-m-1)}
   $$
   拒绝域为
   $$
   \mathscr{X}_0 = \{f>F_{1-\alpha}(m, n-m-1)\}
   $$
   

 
